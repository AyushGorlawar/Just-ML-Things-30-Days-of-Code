# Just-ML-Things-30-Days-of-Code
A personal 30-day journey to sharpen my Machine Learning skills through daily coding

---

# Day 1 (20-06-2025): Intro to ML + Setup
âœ“ Today I brushed up the basics of Machine Learning â€” what it is, its types (Supervised, Unsupervised, Reinforcement), and how ML fits into the world of data science and AI.

âœ“ I also set up my ML environment with Python, Jupyter Notebook, and essential libraries like numpy, pandas, matplotlib, seaborn, and scikit-learn.

## Created three basic files:
âœ“ - `what_is_ml.md` â€“ Theory and introduction  
âœ“ - `types_of_ml.md` â€“ Categorization with examples  
âœ“ - `setup.py` â€“ Environment test + sample code

---

# Day 2 (25-06-2025): Linear Regression
âœ“ Today I explored **Linear Regression**, one of the simplest and most powerful algorithms in supervised learning.

âœ“ I learned how to fit a line to data using `scikit-learn`, understand the slope and intercept, and evaluate the model using **RÂ² score**.

## Created two files:
âœ“ - `linear_regression_theory.md` â€“ Core concept, equations, and use cases  
âœ“ - `linear_regression_sklearn.py` â€“ Hands-on implementation using a toy dataset (X vs y)

ğŸ“Š Bonus: Also plotted the regression line to visualize how well the model fits the data.

---

# Day 3 (26-06-2025): Multiple Linear Regression
âœ“ Today I implemented **Multiple Linear Regression** to predict a target variable based on **two or more input features**.

âœ“ Learned how to prepare a feature matrix using `pandas`, fit the model with `LinearRegression()` from `scikit-learn`, and interpret the learned coefficients.

âœ“ Also evaluated the model performance using **RÂ² score** and predicted outputs for new inputs.

## Created two files:
âœ“ - `multiple_linear_regression_theory.md` â€“ Concept, formula (`y = bâ‚€ + bâ‚xâ‚ + bâ‚‚xâ‚‚ + ...`), and real-life applications  
âœ“ - `multiple_linear_regression_sklearn.py` â€“ Code using `scikit-learn` with custom input features

ğŸ“Š Bonus: Plotted Actual vs Predicted values to visually assess model fit.

---

# Day 4 (27-06-2025): Polynomial Regression
âœ“ Today I implemented **Polynomial Regression** to model non-linear data using polynomial features.

âœ“ I used `PolynomialFeatures` from `sklearn.preprocessing` to generate higher-degree terms and fit them using `LinearRegression`.

âœ“ Evaluated model performance using **RÂ² score** and visualized how polynomial fitting improves prediction over simple linear regression.

## Created two files:
âœ“ - `polynomial_regression_theory.md` â€“ Theory, when to use polynomial models, and key formulas  
âœ“ - `polynomial_regression_sklearn.py` â€“ Python script with polynomial regression code and visualization

ğŸ“Š Bonus: Compared linear vs polynomial curve fitting and saw significant improvement for non-linear patterns.

---

# Day 5 (28-06-2025): Logistic Regression
âœ“ Today I implemented **Logistic Regression**, which is used for binary classification problems.

âœ“ Explored how the **sigmoid function** converts linear output to probability, and used it to classify outcomes like Admitted (1) or Not (0).

âœ“ Evaluated the model using **confusion matrix** and **classification report**.

## Created two files:
âœ“ - `logistic_regression_theory.md` â€“ Explanation of sigmoid, binary classification, and performance metrics  
âœ“ - `logistic_regression.py` â€“ Code to train and evaluate a logistic regression model using scikit-learn

ğŸ“Š Bonus: Made predictions on new unseen inputs and analyzed the results.

---

# Day 6 (29-06-2025): K-Nearest Neighbors (KNN)
âœ“ Today I implemented **KNN classifier**, a simple yet powerful algorithm based on **distance to nearest neighbors**.

âœ“ Used the classic **Iris dataset**, applied **feature scaling**, and trained the model for `k=3`.

âœ“ Evaluated performance using **classification report**.

## Created two files:
âœ“ - `knn_theory.md` â€“ KNN intuition, distance calculation, and pros/cons  
âœ“ - `knn_classifier.py` â€“ Scikit-learn implementation with Iris dataset and standardization

ğŸ“Š Bonus: Compared results using different values of k and noted the impact.

---

# Day 7 (30-06-2025): Naive Bayes Classifier
âœ“ Today I explored **Naive Bayes**, a probabilistic classifier based on **Bayesâ€™ Theorem**.

âœ“ Used `GaussianNB` for classification on the Iris dataset and learned about when to use Gaussian, Bernoulli, or Multinomial NB.

âœ“ Focused on speed, simplicity, and surprisingly good accuracy for text-like data.

## Created two files:
âœ“ - `naive_bayes_theory.md` â€“ Bayesâ€™ formula, assumptions of independence, and use cases  
âœ“ - `naive_bayes_classifier.py` â€“ Implementation using scikit-learnâ€™s GaussianNB

ğŸ“Š Bonus: Understood why Naive Bayes is so widely used in spam filtering and NLP tasks.

---

# Day 8 (01-07-2025): Decision Tree Classifier
âœ“ Learned about tree-based models that split data using decision rules. Implemented using `DecisionTreeClassifier` on the Iris dataset with a visual tree plot.

## Created two files:
âœ“ - `decision_tree_theory.md`  
âœ“ - `decision_tree_classifier.py`

ğŸ“Š Bonus: Visualized the decision boundaries using `plot_tree()` from `sklearn.tree`.

---

# Day 9 (02-07-2025): Random Forest Classifier
âœ“ Explored the ensemble concept of **Random Forest**, combining multiple trees for more robust predictions. Used 100 trees on the Iris dataset.

## Created two files:
âœ“ - `random_forest_theory.md`  
âœ“ - `random_forest_classifier.py`

ğŸ“Š Bonus: Observed how randomization and aggregation improves accuracy and reduces overfitting.

---

# Day 10 (03-07-2025): Support Vector Machine (SVM)
âœ“ Implemented **SVM** using `SVC` with RBF kernel. Understood margin maximization and kernel tricks. Feature scaling was necessary.

## Created two files:
âœ“ - `svm_theory.md`  
âœ“ - `svm_classifier.py`

ğŸ“Š Bonus: Tried different kernels like linear and poly to see their effect on classification.

---

# Day 11 (04-07-2025): Principal Component Analysis (PCA)
âœ“ Today I explored **PCA**, an unsupervised technique to reduce dimensions while retaining maximum variance.

âœ“ Applied it on the Iris dataset to reduce 4D to 2D and visualized clusters in 2D space.

## Created two files:
âœ“ - `pca_theory.md`  
âœ“ - `pca_dimensionality_reduction.py`

ğŸ“Š Bonus: Checked explained variance ratio to confirm how much information is retained.

---

# Day 12 (05-07-2025): K-Means Clustering
âœ“ Learned about **K-Means**, an algorithm that groups data into K clusters based on feature similarity.

âœ“ Used synthetic data to demonstrate clustering and visualized it with centroid locations.

## Created two files:
âœ“ - `kmeans_theory.md`  
âœ“ - `kmeans_clustering.py`

ğŸ“Š Bonus: Understood how the algorithm converges by updating centroids iteratively.
---

# Day 13 (06-07-2025): Hierarchical Clustering
âœ“ Learned about **Agglomerative Clustering** using dendrograms and linkage methods.

âœ“ Visualized merges with a dendrogram and clustered synthetic data using Ward linkage.

## Created two files:
âœ“ - `hierarchical_clustering_theory.md`  
âœ“ - `hierarchical_clustering.py`

ğŸ“Š Bonus: Observed how clusters are formed in a bottom-up fashion and explored complete vs average linkage.

---

# Day 14 (07-07-2025): Elbow Method + Silhouette Score
âœ“ Implemented both **Elbow Method** and **Silhouette Score** to determine the optimal number of clusters for K-Means.

âœ“ Used synthetic data to show how inertia and silhouette values change with different values of K.

## Created two files:
âœ“ - `clustering_evaluation_theory.md`  
âœ“ - `kmeans_evaluation.py`

ğŸ“Š Bonus: Found K=4 to be optimal for our dataset using both methods.

---

# Day 15 (08-07-2025): Mini Project â€“ Mall Customer Segmentation
âœ“ Clustered real-world **Mall Customer Data** based on annual income and spending score.

âœ“ Found optimal clusters (K=5) using the Elbow Method and visualized them with centroids.

âœ“ Segments were interpretable and could be used for targeted marketing.

## Created two files:
âœ“ - `mall_segmentation_project.md`  
âœ“ - `mall_customer_segmentation.py`

ğŸ“Š Bonus: Identified five distinct customer segments and potential business strategies for each.

# Day 16 (09-07-2025): Credit Card Fraud Detection Using Imbalanced Data Techniques

âœ“ Today I tackled a real-world problem: detecting **fraudulent credit card transactions** from a highly **imbalanced dataset** (~0.17% fraud).

âœ“ Learned to handle imbalance using **SMOTE** (Synthetic Minority Oversampling Technique) and built models with **Logistic Regression**, **Random Forest**, and **XGBoost**.

âœ“ Evaluated models using **Precision, Recall, F1-Score**, and **ROC-AUC** to focus on minimizing false negatives (undetected fraud).

## Created three files:
âœ“ - `fraud_detection_theory.md` â€“ Explained the challenge of imbalanced data, fraud detection techniques, and SMOTE logic
âœ“ - `fraud_detection.ipynb` â€“ Data cleaning, SMOTE balancing, model training + ROC curve
âœ“ - `model_comparison.png` â€“ ROC Curve comparison of all three models

ğŸ“Š Bonus: XGBoost performed the best with an ROC AUC close to **0.98**, making it ideal for high-recall fraud detection systems.

---

## ğŸ“‚ Dataset
- Source: [Kaggle - Credit Card Fraud Detection](https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud)
- 284,807 transactions with anonymized features (`V1` to `V28`) + `Amount` and `Time`
- Target: `Class` â†’ 0 = Legit, 1 = Fraud

---

## ğŸ“ˆ What I Learned
- Imbalanced datasets need special handling â€“ accuracy alone is misleading.
- SMOTE works well to balance data synthetically.
- ROC-AUC and F1-Score are better metrics than accuracy here.
- Fraud detection models must prioritize **Recall** (catching frauds).

# Day 17 (09-07-2025): Handling Missing Values & Outliers

âœ“ Today I explored techniques for handling **missing values** using mean/median imputation.

âœ“ Also learned to detect and remove **outliers** using the IQR method and visualized them using boxplots.

âœ“ - `missing_outlier_handling_theory.md`  
âœ“ - `missing_outlier_handling.py`

ğŸ“Š Bonus: Cleaned a dummy dataset and removed an outlier with age 120!

---

# Day 18 (10-07-2025): Scaling, Normalization & Encoding

âœ“ Learned how to prepare data for ML by **scaling features** and **encoding categorical variables**.

âœ“ Applied `StandardScaler`, `LabelEncoder`, and `OneHotEncoder` to transform data correctly.

âœ“ - `scaling_encoding_theory.md`  
âœ“ - `scaling_encoding.py`

ğŸ“Š Bonus: Observed how scaling changes data distribution and why itâ€™s crucial for models like SVM & KNN.

