# Just-ML-Things-30-Days-of-Code
A personal 30-day journey to sharpen my Machine Learning skills through daily coding

---

# Day 1 (20-06-2025): Intro to ML + Setup
âœ“ Today I brushed up the basics of Machine Learning â€” what it is, its types (Supervised, Unsupervised, Reinforcement), and how ML fits into the world of data science and AI.

âœ“ I also set up my ML environment with Python, Jupyter Notebook, and essential libraries like numpy, pandas, matplotlib, seaborn, and scikit-learn.

## Created three basic files:
âœ“ - `what_is_ml.md` â€“ Theory and introduction  
âœ“ - `types_of_ml.md` â€“ Categorization with examples  
âœ“ - `setup.py` â€“ Environment test + sample code

---

# Day 2 (25-06-2025): Linear Regression
âœ“ Today I explored **Linear Regression**, one of the simplest and most powerful algorithms in supervised learning.

âœ“ I learned how to fit a line to data using `scikit-learn`, understand the slope and intercept, and evaluate the model using **RÂ² score**.

## Created two files:
âœ“ - `linear_regression_theory.md` â€“ Core concept, equations, and use cases  
âœ“ - `linear_regression_sklearn.py` â€“ Hands-on implementation using a toy dataset (X vs y)

ğŸ“Š Bonus: Also plotted the regression line to visualize how well the model fits the data.

---

# Day 3 (26-06-2025): Multiple Linear Regression
âœ“ Today I implemented **Multiple Linear Regression** to predict a target variable based on **two or more input features**.

âœ“ Learned how to prepare a feature matrix using `pandas`, fit the model with `LinearRegression()` from `scikit-learn`, and interpret the learned coefficients.

âœ“ Also evaluated the model performance using **RÂ² score** and predicted outputs for new inputs.

## Created two files:
âœ“ - `multiple_linear_regression_theory.md` â€“ Concept, formula (`y = bâ‚€ + bâ‚xâ‚ + bâ‚‚xâ‚‚ + ...`), and real-life applications  
âœ“ - `multiple_linear_regression_sklearn.py` â€“ Code using `scikit-learn` with custom input features

ğŸ“Š Bonus: Plotted Actual vs Predicted values to visually assess model fit.

---

# Day 4 (27-06-2025): Polynomial Regression
âœ“ Today I implemented **Polynomial Regression** to model non-linear data using polynomial features.

âœ“ I used `PolynomialFeatures` from `sklearn.preprocessing` to generate higher-degree terms and fit them using `LinearRegression`.

âœ“ Evaluated model performance using **RÂ² score** and visualized how polynomial fitting improves prediction over simple linear regression.

## Created two files:
âœ“ - `polynomial_regression_theory.md` â€“ Theory, when to use polynomial models, and key formulas  
âœ“ - `polynomial_regression_sklearn.py` â€“ Python script with polynomial regression code and visualization

ğŸ“Š Bonus: Compared linear vs polynomial curve fitting and saw significant improvement for non-linear patterns.

---

# Day 5 (28-06-2025): Logistic Regression
âœ“ Today I implemented **Logistic Regression**, which is used for binary classification problems.

âœ“ Explored how the **sigmoid function** converts linear output to probability, and used it to classify outcomes like Admitted (1) or Not (0).

âœ“ Evaluated the model using **confusion matrix** and **classification report**.

## Created two files:
âœ“ - `logistic_regression_theory.md` â€“ Explanation of sigmoid, binary classification, and performance metrics  
âœ“ - `logistic_regression.py` â€“ Code to train and evaluate a logistic regression model using scikit-learn

ğŸ“Š Bonus: Made predictions on new unseen inputs and analyzed the results.

---

# Day 6 (29-06-2025): K-Nearest Neighbors (KNN)
âœ“ Today I implemented **KNN classifier**, a simple yet powerful algorithm based on **distance to nearest neighbors**.

âœ“ Used the classic **Iris dataset**, applied **feature scaling**, and trained the model for `k=3`.

âœ“ Evaluated performance using **classification report**.

## Created two files:
âœ“ - `knn_theory.md` â€“ KNN intuition, distance calculation, and pros/cons  
âœ“ - `knn_classifier.py` â€“ Scikit-learn implementation with Iris dataset and standardization

ğŸ“Š Bonus: Compared results using different values of k and noted the impact.

---

# Day 7 (30-06-2025): Naive Bayes Classifier
âœ“ Today I explored **Naive Bayes**, a probabilistic classifier based on **Bayesâ€™ Theorem**.

âœ“ Used `GaussianNB` for classification on the Iris dataset and learned about when to use Gaussian, Bernoulli, or Multinomial NB.

âœ“ Focused on speed, simplicity, and surprisingly good accuracy for text-like data.

## Created two files:
âœ“ - `naive_bayes_theory.md` â€“ Bayesâ€™ formula, assumptions of independence, and use cases  
âœ“ - `naive_bayes_classifier.py` â€“ Implementation using scikit-learnâ€™s GaussianNB

ğŸ“Š Bonus: Understood why Naive Bayes is so widely used in spam filtering and NLP tasks.

---

# Day 8 (01-07-2025): Decision Tree Classifier
âœ“ Learned about tree-based models that split data using decision rules. Implemented using `DecisionTreeClassifier` on the Iris dataset with a visual tree plot.

## Created two files:
âœ“ - `decision_tree_theory.md`  
âœ“ - `decision_tree_classifier.py`

ğŸ“Š Bonus: Visualized the decision boundaries using `plot_tree()` from `sklearn.tree`.

---

# Day 9 (02-07-2025): Random Forest Classifier
âœ“ Explored the ensemble concept of **Random Forest**, combining multiple trees for more robust predictions. Used 100 trees on the Iris dataset.

## Created two files:
âœ“ - `random_forest_theory.md`  
âœ“ - `random_forest_classifier.py`

ğŸ“Š Bonus: Observed how randomization and aggregation improves accuracy and reduces overfitting.

---

# Day 10 (03-07-2025): Support Vector Machine (SVM)
âœ“ Implemented **SVM** using `SVC` with RBF kernel. Understood margin maximization and kernel tricks. Feature scaling was necessary.

## Created two files:
âœ“ - `svm_theory.md`  
âœ“ - `svm_classifier.py`

ğŸ“Š Bonus: Tried different kernels like linear and poly to see their effect on classification.

---

# Day 11 (04-07-2025): Principal Component Analysis (PCA)
âœ“ Today I explored **PCA**, an unsupervised technique to reduce dimensions while retaining maximum variance.

âœ“ Applied it on the Iris dataset to reduce 4D to 2D and visualized clusters in 2D space.

## Created two files:
âœ“ - `pca_theory.md`  
âœ“ - `pca_dimensionality_reduction.py`

ğŸ“Š Bonus: Checked explained variance ratio to confirm how much information is retained.

---

# Day 12 (05-07-2025): K-Means Clustering
âœ“ Learned about **K-Means**, an algorithm that groups data into K clusters based on feature similarity.

âœ“ Used synthetic data to demonstrate clustering and visualized it with centroid locations.

## Created two files:
âœ“ - `kmeans_theory.md`  
âœ“ - `kmeans_clustering.py`

ğŸ“Š Bonus: Understood how the algorithm converges by updating centroids iteratively.
